{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9ntGj1cYDqc"
      },
      "source": [
        "# Artificial IntelligenceApplied toSatellite-based RemoteSensing Data for EarthObservation\n",
        "## Chapter 7 - Example 2: Classification\n",
        "\n",
        "The aim of this project is to implement a model able to classify simple object"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdTCqAi7YLkR"
      },
      "source": [
        "Load the [NumPy](https://numpy.org/) library. NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ArJEvmxXx_n"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lopz0W2cYN8o"
      },
      "source": [
        "Load the [Matplotlib](https://matplotlib.org/) library. Matplotlib is a plotting library for the Python programming language and its numerical mathematics extension NumPy. It provides an object-oriented API for embedding plots into applications using general-purpose GUI toolkits like Tkinter, wxPython, Qt, or GTK+. There is also a procedural \"pylab\" interface based on a state machine (like OpenGL), designed to closely resemble that of MATLAB, though its use is discouraged. SciPy makes use of Matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Da_hNhVYNjk"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3mOsI7oYYyi"
      },
      "source": [
        "Load some functions from [Tensorflow.Keras](https://www.tensorflow.org/api_docs/python/tf/keras) library. Keras is an API designed for human beings, not machines. Keras follows best practices for reducing cognitive load: it offers consistent & simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear & actionable error messages. It also has extensive documentation and developer guides. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiTdAdxjYTFW"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkA8cI8kYoh7"
      },
      "source": [
        "## Create the dataset\n",
        "For this simple project the dataset is created on-line. A common practice in Deep Learning is the use of generators. Basically a generator is composed of an infinite while loop and of the yield keyword. The generator is used to avoid memory problems, infact the datast is not loaded all in once, but batch by batch.\n",
        "\n",
        "Before implementig the generator, the noise function is introduced. This function add noise for each group of 4 pixels, in RGB space. This function adds also a fake object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzPEKuD6LbB_"
      },
      "outputs": [],
      "source": [
        "# Function that generates noise\n",
        "def noise(noise_power=0.4):\n",
        "  x = np.random.randint(4, high = 32 - 4)\n",
        "  y = np.random.randint(4, high = 32 - 4)\n",
        "  \n",
        "  noise = np.random.random((32, 32, 3))*noise_power\n",
        "  noise[x:x+4, y:y+4, ...] = np.random.randint(0, 2, size = 3)\n",
        "\n",
        "  return noise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeXgckGXO57d"
      },
      "source": [
        "The generator, in this case, produces an RGB image of size (32, 32, 3) -> (width, height, channel), in which you can see a random object, the background noise and the fake (noise) object. The position of the objects is random."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkFC3L58YZi2"
      },
      "outputs": [],
      "source": [
        "# Function for generating the dataset\n",
        "def dataset_generator(batch_size = 16):\n",
        "  img_shape = (32, 32, 3)\n",
        "  object_size = 8 \n",
        "\n",
        "  while True:\n",
        "    x_train = np.zeros((batch_size, img_shape[0], img_shape[1], img_shape[2]))\n",
        "    # The classes are represented by the colors\n",
        "    y_train = np.zeros((batch_size, 6))\n",
        "\n",
        "    for b in range(batch_size):\n",
        "\n",
        "      selector = np.random.randint(0, high = 6, size = 1)\n",
        "\n",
        "      x = np.random.randint(object_size, high = img_shape[0] - object_size)\n",
        "      y = np.random.randint(object_size, high = img_shape[1] - object_size)\n",
        "      \n",
        "      # Create and object randomly (random position and random class)\n",
        "      if selector == 1:\n",
        "        # Red object\n",
        "        x_train[b, x:x+object_size, y:y+object_size, 0] = 0.6\n",
        "        y_train[b, 1] = 1.0\n",
        "      elif selector == 2:\n",
        "        # Green object\n",
        "        x_train[b, x:x+object_size, y:y+object_size, 1] = 0.6\n",
        "        y_train[b, 2] = 1.0\n",
        "      elif selector == 3:\n",
        "        # Blue object\n",
        "        x_train[b, x:x+object_size, y:y+object_size, 2] = 0.6\n",
        "        y_train[b, 3] = 1.0\n",
        "      elif selector == 4:\n",
        "        # Yellow object\n",
        "        x_train[b, x:x+object_size, y:y+object_size, 0] = 0.5\n",
        "        x_train[b, x:x+object_size, y:y+object_size, 1] = 0.5\n",
        "        y_train[b, 4] = 1.0\n",
        "      elif selector == 5:\n",
        "        # Fuchsia object\n",
        "        x_train[b, x:x+object_size, y:y+object_size, 0] = 0.5\n",
        "        x_train[b, x:x+object_size, y:y+object_size, 2] = 0.5\n",
        "        y_train[b, 5] = 1.0\n",
        "      else:\n",
        "        # No object\n",
        "        x_train[b, ...] = 0\n",
        "        y_train[b, 0] = 1.0\n",
        "      \n",
        "      # Add noise to the batches\n",
        "      x_train[b, ...] = x_train[b, ...] + noise()\n",
        "      x_train[b, ...] = np.clip(x_train[b, ...], 0.0, 1.0)\n",
        "    \n",
        "    yield x_train, y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K71TUgFkQH-O"
      },
      "source": [
        "To call the generator you should use the following commands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FX7xgm8fLuar"
      },
      "outputs": [],
      "source": [
        "# Initialize the generator\n",
        "generator = dataset_generator(512) \n",
        "\n",
        "# Get the iterator from the generator \n",
        "iterator = iter(generator)\n",
        "\n",
        "# Get a batch of data\n",
        "x_train, y_train = next(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCU2hTebQTg-"
      },
      "source": [
        "Define a vector with a label for each class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWVDiEFTp1W_"
      },
      "outputs": [],
      "source": [
        "classes = ['None', 'Red', 'Green', 'Blue', 'Yellow', 'Fuchsia']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcxqy-9tQh4M"
      },
      "source": [
        "Let plot an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 934
        },
        "id": "H2L2djvvLorw",
        "outputId": "23a2c2ee-44ca-460f-c5cf-face40d5a726"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows=3, ncols=3, figsize = (16, 16))\n",
        "counter = 0\n",
        "for i in range(3):\n",
        "  for j in range(3):\n",
        "    current =  y_train[counter, ...].tolist()\n",
        "    c = classes[current.index(max(current))]\n",
        "    axes[i,j].imshow(x_train[counter,...], cmap = 'gray')\n",
        "    axes[i,j].set_title('Class: [%1.1f %1.1f %1.1f %1.1f %1.1f %1.1f]\\n %s'  \n",
        "                        % (y_train[counter, 0], y_train[counter, 1], y_train[counter, 2], y_train[counter, 3], y_train[counter, 4], y_train[counter, 5],c), fontsize=14)\n",
        "    axes[i,j].axis(False)\n",
        "\n",
        "    counter = counter + 1\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3dZupmSQtKE"
      },
      "source": [
        "# Build a model\n",
        "\n",
        "The model is composed of different Dense layer, with a decreasing size, the ReLu as activation function (except for the last layer). As you can see before the first dense layer there is a Flatten layer. Since the input is a 3D matrix of size 32x32x3 and since the Dense layer is expecting a 1D vector, the flatten layer is necessary. This layer uroll the matrix in a 1D vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gd7uRSZWLs3w"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(32, 32, 3)))\n",
        "model.add(Dense(512, activation = 'relu'))\n",
        "model.add(Dense(128, activation = 'relu'))\n",
        "model.add(Dense(64, activation = 'relu'))\n",
        "model.add(Dense(16, activation = 'relu'))\n",
        "model.add(Dense(6, activation = 'softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UL4InITRo4O"
      },
      "source": [
        "Compile the model using the Binary Crossentropy as loss function and Adam as optimizer, and accuracy as extra metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZay3pYKRksj"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D72_qSlTR1mQ"
      },
      "source": [
        "Print the model structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "zXainPlDR1PE",
        "outputId": "ee26d2da-7931-492b-c5b0-91ae76881cb5"
      },
      "outputs": [],
      "source": [
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q68pqeimSEV1"
      },
      "source": [
        "# Model training\n",
        "The model will be trained with the function fit_generator, for 10 epcohs. This function uses generators, both for training and validations set. Since the generator are producing an infiniry of samples, using steps_per_epoch and validation_step, the user can define an arbitrary size for the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "y4PO3OlRR5g4",
        "outputId": "e945f4fc-bdc5-46df-b8a4-2a42956d993c"
      },
      "outputs": [],
      "source": [
        "history = model.fit_generator(\n",
        "    dataset_generator(16),\n",
        "    steps_per_epoch=1000//16,\n",
        "    validation_data=dataset_generator(8),\n",
        "    validation_steps=100//8,\n",
        "    epochs = 10\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayRh3dKoSPPK"
      },
      "source": [
        "Plot the training score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 914
        },
        "id": "Yn6Vb0c4SMQZ",
        "outputId": "7a45093c-0ac6-4ca8-88e7-7d26e3cddde7"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(nrows=2, ncols=1, figsize = (15, 15))\n",
        "ax[0].plot(history.history['loss'], label = 'Training Loss')\n",
        "ax[0].plot(history.history['val_loss'], label = 'Validation Loss')\n",
        "\n",
        "ax[0].set_title('Training VS Validation Loss', fontsize = 28)\n",
        "ax[0].set_xlabel('Epochs', fontsize = 20)\n",
        "ax[0].set_ylabel('MAE', fontsize = 20)\n",
        "ax[0].legend(fontsize = 20)\n",
        "\n",
        "ax[1].plot(history.history['accuracy'], label = 'Training accuracy')\n",
        "ax[1].plot(history.history['val_accuracy'], label = 'Validation accuracy')\n",
        "\n",
        "ax[1].set_title('Training VS Validation Accuracy', fontsize = 28)\n",
        "ax[1].set_xlabel('Epochs', fontsize = 20)\n",
        "ax[1].set_ylabel('Accuracy', fontsize = 20)\n",
        "ax[1].legend(fontsize = 20)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdQ9VzCHSudc"
      },
      "source": [
        "# Test the trained model\n",
        "\n",
        "Load new samples through the generator and predict the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ricfnv0mSblP"
      },
      "outputs": [],
      "source": [
        "x_train, y_train = next(iter(dataset_generator(9)))\n",
        "pred = model.predict(x_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5B4wpNLTJU8"
      },
      "source": [
        "Plot the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 934
        },
        "id": "fmt2lpaDSbzl",
        "outputId": "938965e5-709f-4c08-dfb2-784efd134cb6"
      },
      "outputs": [],
      "source": [
        "ig, axes = plt.subplots(nrows=3, ncols=3, figsize = (16, 16))\n",
        "counter = 0\n",
        "for i in range(3):\n",
        "  for j in range(3):\n",
        "\n",
        "    current_real =  y_train[counter, ...].tolist()\n",
        "    c_real = classes[current_real.index(max(current_real))]\n",
        "\n",
        "    current_pred =  pred[counter, ...].tolist()\n",
        "    c_pred = classes[current_pred.index(max(current_pred))]\n",
        "\n",
        "\n",
        "\n",
        "    axes[i,j].imshow(x_train[counter,...], cmap = 'gray')\n",
        "    axes[i,j].set_title('Real: [%1.1f %1.1f %1.1f %1.1f %1.1f %1.1f] -> %s\\nPred: [%1.1f %1.1f %1.1f %1.1f %1.1f %1.1f] -> %s'  \n",
        "                        % (y_train[counter, 0], y_train[counter, 1], y_train[counter, 2], y_train[counter, 3],\n",
        "                            y_train[counter, 4], y_train[counter, 5], c_real ,pred[counter,0], pred[counter, 1],\n",
        "                            pred[counter, 2], pred[counter, 3], pred[counter, 4], pred[counter, 5], c_pred), fontsize=14)\n",
        "    axes[i,j].axis(False)\n",
        "    counter = counter + 1\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxxQUGYAWCA4"
      },
      "source": [
        "# Confusion matrix\n",
        "\n",
        "In the field of machine learning and specifically the problem of statistical classification, a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix), also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one (in unsupervised learning it is usually called a matching matrix). Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa). The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabeling one as another).\n",
        "\n",
        "It is a special kind of contingency table, with two dimensions (\"actual\" and \"predicted\"), and identical sets of \"classes\" in both dimensions (each combination of dimension and class is a variable in the contingency table). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_F2K_4ZXIRm"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxxvLNGwVN5v"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "  \"\"\"\n",
        "  This function prints and plots the confusion matrix.\n",
        "  Normalization can be applied by setting `normalize=True`.\n",
        "  \"\"\"\n",
        "  import itertools\n",
        "  if normalize:\n",
        "      cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "      print(\"Normalized confusion matrix\")\n",
        "  else:\n",
        "      print('Confusion matrix, without normalization')\n",
        "\n",
        "\n",
        "  plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "  plt.title(title, fontsize = 28)\n",
        "  plt.colorbar()\n",
        "  tick_marks = np.arange(len(classes))\n",
        "  plt.xticks(tick_marks, classes, rotation=45, fontsize = 16)\n",
        "  plt.yticks(tick_marks, classes, fontsize = 16)\n",
        "\n",
        "  fmt = '.2f' if normalize else 'd'\n",
        "  thresh = cm.max() / 2.\n",
        "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "      plt.text(j, i, format(cm[i, j], fmt),\n",
        "                horizontalalignment=\"center\",\n",
        "                color=\"white\" if cm[i, j] > thresh else \"black\", fontsize = 16)\n",
        "\n",
        "  plt.ylabel('True label', fontsize = 20)\n",
        "  plt.xlabel('Predicted label', fontsize = 20)\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "# Get the labels from vector one hot encoded\n",
        "def getLabels(one_hot_encoded):\n",
        "  l = np.argmax(one_hot_encoded, axis=1)\n",
        "  labels = []\n",
        "\n",
        "  for i in range(one_hot_encoded.shape[0]):\n",
        "    if l[i] == 0:\n",
        "      labels.append('None')\n",
        "    elif l[i] == 1:\n",
        "      labels.append('Red')\n",
        "    elif l[i] == 2:\n",
        "      labels.append('Green')\n",
        "    elif l[i] == 3:\n",
        "      labels.append('Blue')\n",
        "    elif l[i] == 4:\n",
        "      labels.append('Yellow')\n",
        "    elif l[i] == 5:\n",
        "      labels.append('Fuchsia')\n",
        "\n",
        "  return labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OA-3De3Ygjk"
      },
      "source": [
        "Get more data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eFCZlyxWX7v"
      },
      "outputs": [],
      "source": [
        "x_train, y_train = next(iter(dataset_generator(2048)))\n",
        "pred = model.predict(x_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezSCejvsYiF0"
      },
      "source": [
        "Plot the confusion matrices (not normalized and normalized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mUtE6WgiWFDq",
        "outputId": "ccce1db2-bc5b-43f9-f405-f55421421c3a"
      },
      "outputs": [],
      "source": [
        "pl = getLabels(pred)\n",
        "cl = getLabels(y_train)\n",
        "\n",
        "cm = confusion_matrix(pl, cl, classes)\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plt.figure(figsize=(16, 12))\n",
        "plot_confusion_matrix(cm, classes=classes, title='Confusion matrix, without normalization\\n')\n",
        "\n",
        "# Plot normalized confusion matrix\n",
        "plt.figure(figsize=(16, 12))\n",
        "plot_confusion_matrix(cm, classes=classes, normalize=True, title='Confusion matrix, with normalization\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09fI9zSOXY3H"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
